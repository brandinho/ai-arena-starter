{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Arena Starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandinho/ai-arena-starter/blob/main/AI_Arena_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start off, we will install the `aiarena-gym` package"
      ],
      "metadata": {
        "id": "6j7SEeFfCh6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiarena-gym"
      ],
      "metadata": {
        "id": "KVgwB5Gsuai1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will import all of the packages that we'll need to run a sample model in the AI Arena gym. \n",
        "<br><br>\n",
        "We will build our sample model with PyTorch."
      ],
      "metadata": {
        "id": "py8qUZGzCrKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EowHT4l9NHRj"
      },
      "outputs": [],
      "source": [
        "from aiarena_gym.environment import Game\n",
        "from aiarena_gym.benchmarks.rules_based_agents import opponents\n",
        "from aiarena_gym.helpers.exporting_ops import save_pytorch_model\n",
        "from aiarena_gym.helpers.game_functions import get_state\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will create a simple function to randomly sample minibatches from our dataset"
      ],
      "metadata": {
        "id": "ktSOsEVwEkq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_shuffled_index(n_observations):\n",
        "    return np.random.choice(n_observations, size = n_observations, replace = False)\n",
        "    \n",
        "\n",
        "def sample_minibatches(data_tuple, instance_axis_tuple, batch_size):\n",
        "    assert all(\n",
        "        [data_tuple[x].shape[ax] == data_tuple[0].shape[instance_axis_tuple[0]] \n",
        "         for x, ax in zip(range(1, len(data_tuple)), instance_axis_tuple[1:])]\n",
        "    )\n",
        "\n",
        "    # This assumes that the first entry in the tuple is always the inputs\n",
        "    n_observations = data_tuple[0].shape[0]\n",
        "    n_batches = n_observations // batch_size + 1\n",
        "    \n",
        "    shuffled_index = get_shuffled_index(n_observations)\n",
        "\n",
        "    batch_num = 0\n",
        "    for _ in range(n_batches):\n",
        "        current_index = shuffled_index[batch_num:batch_num+batch_size]\n",
        "        yield (x[current_index] if ax == 0 else x[:,current_index] for x, ax in zip(data_tuple, instance_axis_tuple))\n",
        "        batch_num += batch_size"
      ],
      "metadata": {
        "id": "f2V8B4rwEM_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will define a simple policy gradient, which trains via the REINFORCE algorithm. The training function uses the minibatch sampling that we defined above."
      ],
      "metadata": {
        "id": "s_UjcD3Jrvk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyGradient(torch.nn.Module):\n",
        "    def __init__(self, n_features, n_actions, neurons, activation_function, learning_rate):\n",
        "        super(PolicyGradient, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_actions = n_actions\n",
        "        self.neurons = neurons\n",
        "        self.activation_function = activation_function\n",
        "        self.learning_rate = learning_rate\n",
        "                \n",
        "        self.output_activation = F.softmax\n",
        "        \n",
        "        self.n_layers = len(self.neurons) + 1\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for l in range(self.n_layers):\n",
        "            if l == 0:\n",
        "                in_dim = n_features\n",
        "                out_dim = neurons[l]\n",
        "            elif l == self.n_layers - 1:\n",
        "                in_dim = neurons[l-1]\n",
        "                out_dim = n_actions\n",
        "            else:\n",
        "                in_dim = neurons[l-1]\n",
        "                out_dim = neurons[l]                \n",
        "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
        "            \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n",
        "    \n",
        "    def policy(self, state):\n",
        "        current_layer = state\n",
        "        for l in range(self.n_layers):\n",
        "            if l < self.n_layers - 1:\n",
        "                current_layer = self.activation_function(self.layers[l](current_layer))\n",
        "            else:\n",
        "                current_layer = self.output_activation(self.layers[l](current_layer), dim = 1)\n",
        "        return current_layer\n",
        "        \n",
        "    @staticmethod\n",
        "    def tensor_to_array(torch_tensor):\n",
        "        return torch_tensor.detach().cpu().numpy()\n",
        "        \n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state).float()\n",
        "        policy = self.tensor_to_array(self.policy(state))\n",
        "        action = np.random.choice(\n",
        "            np.arange(self.n_actions), \n",
        "            1, \n",
        "            p = policy.reshape(-1)\n",
        "        )[0]\n",
        "        return action\n",
        "\n",
        "    def get_loss(self, states, actions, rewards):\n",
        "        states = torch.tensor(states).float()\n",
        "        actions = torch.tensor(actions).type(torch.LongTensor)\n",
        "        rewards = torch.tensor(rewards).float()\n",
        "        \n",
        "        policy = self.policy(states)\n",
        "        actions_one_hot = F.one_hot(actions, num_classes = self.n_actions)\n",
        "        action_probabilities = torch.sum(policy * actions_one_hot, dim = 1)\n",
        "        return -torch.mean(rewards * torch.log(action_probabilities + 0.001))\n",
        "\n",
        "    def train(self, states, actions, rewards, epochs, batch_size, verbose = False):\n",
        "        for i in range(epochs):\n",
        "            instance_axis_tuple = (0,0,0)\n",
        "            data_tuple = (states, actions, rewards)\n",
        "            for minibatch_s, minibatch_a, minibatch_r in sample_minibatches(\n",
        "                data_tuple, \n",
        "                instance_axis_tuple, \n",
        "                batch_size\n",
        "            ):\n",
        "                if len(minibatch_s) == 0:\n",
        "                    continue\n",
        "    \n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.get_loss(minibatch_s, minibatch_a, minibatch_r)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "            if verbose and (i+1) % 5 == 0:\n",
        "                print(\"Epoch {}\".format(i+1))"
      ],
      "metadata": {
        "id": "Qr_lZJLxrhu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will define a simple mapping for activation functions to allow users to select an activation function through the dropdown in the following section."
      ],
      "metadata": {
        "id": "bmTkdkZLuOaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activation_function_mapping = {\n",
        "    \"relu\": F.relu,\n",
        "    \"elu\": F.elu,\n",
        "    \"tanh\": F.tanh\n",
        "}"
      ],
      "metadata": {
        "id": "x3EiFw59uQkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize your model. Note that AI Arena has done feature engineering for this competition, so researchers are unable to change the input dimensionality for the neural network. To learn more about the state space, please check out our [researcher wiki](https://www.notion.so/AI-Arena-State-Space-88ac2ff5e2f14f67a8dbfc703592be50). Additionally, there is a preset number of actions."
      ],
      "metadata": {
        "id": "Gy9-gCB0tjrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model Hyperparameters { form-width: \"400px\" }\n",
        "n_features = 9 # DO NOT CHANGE\n",
        "n_actions = 10 # DO NOT CHANGE\n",
        "neurons = [36, 24, 12]\n",
        "activation_function = \"relu\" #@param [\"relu\", \"elu\", \"tanh\"]\n",
        "learning_rate = 0.0018 #@param {type:\"slider\", min:0, max:0.01, step:0.0001}\n",
        "activation = activation_function_mapping[activation_function]\n",
        "model = PolicyGradient(\n",
        "    n_features, \n",
        "    n_actions,\n",
        "    neurons, \n",
        "    activation,\n",
        "    learning_rate\n",
        ")\n"
      ],
      "metadata": {
        "id": "RY1HTUzftlqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, you have two opponents to choose from. In the researcher platform, sidai corresponds to benchmark 1, and sihing corresponds to benchmark 2. However, be careful not to overfit to these benchmarks because it might not work too well as you try and battle the other models on the leaderboard!"
      ],
      "metadata": {
        "id": "d1Zg1R2buzyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Opponent { form-width: \"400px\" }\n",
        "heuristic_agent = \"Sidai\" #@param [\"Sidai\", \"Sihing\"]\n",
        "opponent_model = opponents[heuristic_agent]()"
      ],
      "metadata": {
        "id": "5HWTnCOHu1a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the battle attributes for your fighter and for your opponent's fighter, and then initialize the game environment!"
      ],
      "metadata": {
        "id": "tY7atufJ6Dnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setting Environment Params { form-width: \"400px\" }\n",
        "\n",
        "#@markdown Your Fighter's Attributes\n",
        "your_power = 69 #@param {type:\"slider\", min:10, max:100, step:1}\n",
        "your_speed = 37 #@param {type:\"slider\", min:10, max:100, step:1}\n",
        "your_defence = 50 #@param {type:\"slider\", min:10, max:100, step:1}\n",
        "your_accuracy = 50 #@param {type:\"slider\", min:10, max:100, step:1}\n",
        "\n",
        "#@markdown Opponent's Attributes\n",
        "opponent_power = 74 #@param {type:\"slider\", min:10, max:100, step:1}\n",
        "opponent_speed = 50 #@param {type:\"slider\", min:10, max:100, step:1}\n",
        "opponent_defence = 50 #@param {type:\"slider\", min:10, max:100, step:1}\n",
        "opponent_accuracy = 50 #@param {type:\"slider\", min:10, max:100, step:1}\n",
        "\n",
        "your_attributes = {\n",
        "    \"power\": your_power,\n",
        "    \"speed\": your_speed,\n",
        "    \"defence\": your_defence,\n",
        "    \"accuracy\": your_accuracy,\n",
        "}\n",
        "opponent_attributes = {\n",
        "    \"power\": opponent_power,\n",
        "    \"speed\": opponent_speed,\n",
        "    \"defence\": opponent_defence,\n",
        "    \"accuracy\": opponent_accuracy,\n",
        "}\n",
        "\n",
        "env = Game()\n",
        "env.load_fighters(model, opponent_model, your_attributes, opponent_attributes)\n"
      ],
      "metadata": {
        "id": "hJ2cBbh76FC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will define a few functions that will help us run reinforcement learning. \n",
        "<br><br>\n",
        "To start, we need to be able to run a game loop, which we define with the `run_battle` function. The key to running the game loop is by iteratively taking steps in the environment until the game is done. You do this by selecting an action and running `env.step(action)`, where `env` is your initialized game environment.\n",
        "<br><br>\n",
        "Next, in order to use reinforcement learning, we need to collect rewards from the environment. In this example, we show an approach for a customized reward function that uses both myopic rewards (immediate impact) and discountable rewards (delayed impact).\n",
        "<br><br>\n",
        "We put these functions together in the `reinforcement_learning` function for a prespecified number of episodes."
      ],
      "metadata": {
        "id": "-pYsTM-b73mZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reward(action_name, your_state, your_new_state, opponent_state, opponent_new_state, winner):    \n",
        "    opponent_health_delta = opponent_new_state[\"health\"]- opponent_state[\"health\"]\n",
        "    your_health_delta = your_new_state[\"health\"] - your_state[\"health\"]\n",
        "    prior_distance = your_state[\"x\"] - opponent_state[\"x\"]\n",
        "    new_distance = your_new_state[\"x\"] - opponent_new_state[\"x\"]\n",
        "    move_bool = \"Left\" in action_name or \"Right\" in action_name\n",
        "    move_closer_bool = abs(prior_distance) > abs(new_distance)\n",
        "    move_away_bool = abs(prior_distance) < abs(new_distance)\n",
        "    \n",
        "    hit_reward = (opponent_health_delta < 0) * 0.3\n",
        "    get_hit_reward = (your_health_delta < 0) * -0.3\n",
        "    move_closer_reward = (move_closer_bool and move_bool) * 0.1\n",
        "    move_away_reward = (move_away_bool and move_bool) * -0.1\n",
        "    \n",
        "    result_reward = 0\n",
        "    if winner == \"You\":\n",
        "        result_reward = 2\n",
        "    elif winner == \"Opponent\":\n",
        "        result_reward = -2\n",
        "    return [result_reward + hit_reward + get_hit_reward, move_closer_reward + move_away_reward]\n",
        "        \n",
        "    \n",
        "def run_battle(env, randomize_attributes = False, random_policy = False):\n",
        "    done = False\n",
        "    all_actions_done = {}\n",
        "    data_collection = {\"s\": [], \"a\": [], \"r\": {\"discountable\": [], \"myopic\": []}}\n",
        "    your_state, opponent_state = env.reset(randomize_attributes, random_policy)\n",
        "    your_attributes = env.your_fighter[\"battle_attributes\"]\n",
        "    opponent_attributes = env.opponent_fighter[\"battle_attributes\"]\n",
        "    state = get_state(your_state, opponent_state, your_attributes, opponent_attributes)\n",
        "    while not done:\n",
        "        action = env.fighters[0][\"model\"].select_action(state)\n",
        "        your_new_state, opponent_new_state, done, winner = env.step(action)\n",
        "        \n",
        "        action_name = env.actions_list[action]\n",
        "        if action_name not in all_actions_done:\n",
        "            all_actions_done[action_name] = 1\n",
        "        else:\n",
        "            all_actions_done[action_name] += 1\n",
        "        d_reward, m_reward = get_reward(action_name, your_state, your_new_state, opponent_state, opponent_new_state, winner)\n",
        "\n",
        "        new_state = get_state(your_new_state, opponent_new_state, your_attributes, opponent_attributes)\n",
        "        \n",
        "        data_collection[\"s\"].append(state[0])\n",
        "        data_collection[\"a\"].append(action)\n",
        "        data_collection[\"r\"][\"discountable\"].append(d_reward)\n",
        "        data_collection[\"r\"][\"myopic\"].append(m_reward)\n",
        "        your_state = your_new_state.copy()\n",
        "        opponent_state = opponent_new_state.copy()\n",
        "        state = new_state.copy()\n",
        "\n",
        "    return winner, data_collection\n",
        "\n",
        "\n",
        "def get_full_reward(rewards, idx):\n",
        "    return rewards[\"discountable\"][idx] + rewards[\"myopic\"][idx]\n",
        "\n",
        "\n",
        "def get_discounted_return(rewards, gamma):\n",
        "    running_discounted_reward = get_full_reward(rewards, len(rewards[\"discountable\"])-1)\n",
        "    discounted_return = rewards[\"discountable\"].copy()\n",
        "    discounted_return[-1] = rewards[\"discountable\"][-1]\n",
        "    for t in reversed(range(len(discounted_return) - 1)):\n",
        "        discounted_return[t] = get_full_reward(rewards, t) + gamma * running_discounted_reward\n",
        "        running_discounted_reward += rewards[\"discountable\"][t]\n",
        "    return np.array(discounted_return)\n",
        "\n",
        "def reinforcement_learning(env, episodes = 100, randomize_attributes = False):\n",
        "    epochs = 10\n",
        "    batch_size = 24\n",
        "    gamma = 0.95\n",
        "    for e in range(episodes):\n",
        "        winner, gameplay_data = run_battle(env, randomize_attributes)\n",
        "        \n",
        "        states = np.array(gameplay_data[\"s\"])\n",
        "        actions = np.array(gameplay_data[\"a\"])\n",
        "        discounted_return = get_discounted_return(gameplay_data[\"r\"], gamma)\n",
        "\n",
        "        env.fighters[0][\"model\"].train(states, actions, discounted_return, epochs, batch_size)\n",
        "\n",
        "        if (e + 1) % 1 == 0:\n",
        "            print(\"Episode: {}, Winner: {}, Return: {}\".format(e+1, winner, np.mean(discounted_return)))"
      ],
      "metadata": {
        "id": "ZFZ1mOsx756n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have defined all the functions, run the training loop and see what your agent has learned!"
      ],
      "metadata": {
        "id": "bFYEw_vk7rXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Simulation Params { form-width: \"400px\" }\n",
        "num_episodes = 2812 #@param {type:\"slider\", min:50, max:5000, step:1}\n",
        "reinforcement_learning(env, num_episodes)"
      ],
      "metadata": {
        "id": "azjx00N27t0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once training is done, you can save your model as a JSON. We have created a number of templates to make exporting easier for you, but if you want to create your own template, please follow our exporting configurations [here](https://www.notion.so/Code-Compliance-6f4ec6c2fda64e268ca4581c4f5b3b67)."
      ],
      "metadata": {
        "id": "qger-RS37f9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "pathlib.Path('/content/saved_model').mkdir(parents=False, exist_ok=True) \n",
        "save_pytorch_model(model)"
      ],
      "metadata": {
        "id": "q7jG1IU67hbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}